---
layout:     post
title:      "RAG手工创建"
subtitle:   " \"文本分片，embedding、存储向量DB，Chromadb，召回、重排，生成\""
date:       2025-10-04 12:00:00
author:     "Hux"
header-img: "img/post-bg-2015.jpg"
catalog: true
tags:
    - Meta
---

> “Yeah It's on. ”


代码：

from typing import List
from sentence_transformers import SentenceTransformer
import chromadb

chromeadb_client = chromadb.EphemeralClient()
collection = None


def split_into_chunks(doc_file) -> List[str]:
    chunks = []
    with open(doc_file, 'r', encoding='utf-8') as file:
        content = file.read()

    for chunk in content.split('\n\n'):
        # print(f"-- {chunk}")
        chunks.append(chunk)
    return chunks


def split_into_chunks2(doc_file) -> List[str]:
    chunks = []
    with open(doc_file, 'r', encoding='utf-8') as file:
        content = file.read()

    # 调试信息
    print(f"文件内容前100字符: {repr(content[:100])}")
    print(f"\\n 数量: {content.count('\n')}")
    print(f"\\n\\n 数量: {content.count('\n\n')}")

    for chunk in content.split('\n\n'):
        print(f"-- {repr(chunk)}")  # 使用repr显示实际字符
        chunks.append(chunk)
    return chunks


def embed_chunk(chunk) -> List[float]:
    embedding = SentenceTransformer("./text2vec-base-chinese").encode(chunk, normalize_embeddings=True)
    return embedding.tolist()


def save_embeddings(chunks: List[str], embeddings: List[List[float]]) -> None:
    global collection
    collection = chromeadb_client.get_or_create_collection(name="default")
    ids = []
    for i in range(len(chunks)):
        ids.append(str(i))
    collection.add(ids=ids, embeddings=embeddings, documents=chunks)
    print("save_embeddings--》", "done")


def retrieve(query: str, top_k: int)-> List[str]:
    query_embedding = embed_chunk(query)
    global collection
    results = collection.query(query_embeddings=query_embedding, n_results=top_k)
    return results["documents"][0]


if __name__ == '__main__':
    # 1.字符串按两个回车切分，切分10个片段
    chunks = split_into_chunks("doc.md")
    print("字符串按两个回车切分，切分list chunks----》", chunks)
    print(len(chunks))
    # 2.把所有list编码
    embedding = embed_chunk(chunks)
    print("2.把所有字符串编码,能看出来10个list ", embedding)
    print(len(embedding))
    # 3.把list分批编码，然后分到10个不同数组
    embeddings = []
    for chunk in chunks:
        embeddings.append(embed_chunk(chunk))
    # print("把list分批编码，然后分到10个不同数组", embedding)
    # 4.存储向量数据库，ChromeDB,并建立索引
    save_embeddings(chunks, embeddings)
    # 5.查询，召回
    query = "哆啦A梦使用的3个秘密道具分别是什么？"
    retrieved_chunks = retrieve(query, 5)
    # for chunk in retrieved_chunks:
    #     print(chunk)

    for i, chunk in enumerate(retrieved_chunks):
        print(f"[{i}] {chunk}")

    # 6.重排序rerank，需要一个重排大模型



