---
layout:     post
title:      "RAG手工创建"
subtitle:   " \"文本分片，embedding、存储向量DB，Chromadb，召回、重排，生成\""
date:       2025-10-04 12:00:00
author:     "Hux"
header-img: "img/post-bg-2015.jpg"
catalog: true
tags:
    - Meta
---

```python
from typing import List
from sentence_transformers import SentenceTransformer
import chromadb

# Initialize ChromaDB client (EphemeralClient creates an in-memory database)
# For persistent storage, consider using chromadb.PersistentClient(path="path/to/db")
chromadb_client = chromadb.EphemeralClient()
collection = None


def split_into_chunks(doc_file) -> List[str]:
    """Splits a document into chunks based on double newlines."""
    chunks = []
    try:
        with open(doc_file, 'r', encoding='utf-8') as file:
            content = file.read()
    except FileNotFoundError:
        print(f"Error: Document file '{doc_file}' not found.")
        return []

    for chunk in content.split('\n\n'):
        if chunk.strip(): # Avoid adding empty chunks
            chunks.append(chunk.strip())
    return chunks


def split_into_chunks2(doc_file) -> List[str]:
    """Splits a document into chunks based on double newlines, with debug prints."""
    chunks = []
    try:
        with open(doc_file, 'r', encoding='utf-8') as file:
            content = file.read()
    except FileNotFoundError:
        print(f"Error: Document file '{doc_file}' not found.")
        return []

    # Debug information
    print(f"文件内容前100字符: {repr(content[:100])}")
    print(f"\\n 数量: {content.count('\n')}")
    print(f"\\n\\n 数量: {content.count('\n\n')}")

    for chunk in content.split('\n\n'):
        if chunk.strip(): # Avoid adding empty chunks
            print(f"-- {repr(chunk.strip())}")  # 使用repr显示实际字符
            chunks.append(chunk.strip())
    return chunks


def embed_chunk(chunk: str) -> List[float]:
    """Embeds a single text chunk using a SentenceTransformer model."""
    # Note: This assumes the model './text2vec-base-chinese' is downloaded locally.
    # For production, consider using a model name directly from the SentenceTransformer library
    # or ensuring the model is properly downloaded and accessible.
    try:
        model = SentenceTransformer("./text2vec-base-chinese")
        embedding = model.encode(chunk, normalize_embeddings=True)
        return embedding.tolist()
    except Exception as e:
        print(f"Error embedding chunk: {e}")
        # Return a zero vector or handle error appropriately
        return [0.0] * 768 # Assuming a common embedding dimension, adjust if needed


def save_embeddings(chunks: List[str], embeddings: List[List[float]]) -> None:
    """Saves chunks and their embeddings to ChromaDB."""
    global collection
    if not collection:
        collection = chromeadb_client.get_or_create_collection(name="default")
    
    if len(chunks) != len(embeddings):
        print("Error: Mismatch between number of chunks and embeddings.")
        return

    ids = [str(i) for i in range(len(chunks))]
    
    # ChromaDB's add method expects documents, embeddings, and ids.
    # Ensure all are provided and match in length.
    try:
        collection.add(ids=ids, embeddings=embeddings, documents=chunks)
        print("save_embeddings--》", "done")
    except Exception as e:
        print(f"Error saving embeddings to ChromaDB: {e}")


def retrieve(query: str, top_k: int)-> List[str]:
    """Retrieves relevant chunks from ChromaDB based on a query."""
    global collection
    if not collection:
        print("Error: ChromaDB collection not initialized.")
        return []
        
    try:
        query_embedding = embed_chunk(query)
        results = collection.query(query_embeddings=[query_embedding], n_results=top_k) # query_embeddings expects a list
        if results and "documents" in results and results["documents"]:
            return results["documents"][0]
        else:
            return []
    except Exception as e:
        print(f"Error during retrieval: {e}")
        return []


if __name__ == '__main__':
    # 1. Split document into chunks
    # Using split_into_chunks for the main logic. split_into_chunks2 is for debugging.
    doc_file_path = "doc.md" # Ensure doc.md exists in the same directory or provide a full path
    chunks = split_into_chunks(doc_file_path)
    print(f"--- Split into {len(chunks)} chunks ---")
    # print("Chunks:", chunks) # Uncomment to see all chunks

    if not chunks:
        print("No chunks found. Exiting.")
    else:
        # 2. Embed all chunks
        print("Embedding chunks...")
        embeddings = []
        for i, chunk in enumerate(chunks):
            print(f"Embedding chunk {i+1}/{len(chunks)}...")
            embeddings.append(embed_chunk(chunk))
        print(f"--- Embedded {len(embeddings)} chunks ---")

        # 3. Save embeddings to ChromaDB
        save_embeddings(chunks, embeddings)

        # 4. Query and retrieve
        query = "哆啦A梦使用的3个秘密道具分别是什么？"
        print(f"\n--- Querying for: '{query}' ---")
        retrieved_chunks = retrieve(query, 5)
        
        if retrieved_chunks:
            print("--- Retrieved chunks ---")
            for i, chunk in enumerate(retrieved_chunks):
                print(f"[{i}] {chunk}")
        else:
            print("No chunks retrieved.")

        # 5. Reranking (placeholder for future implementation)
        # print("\n--- Reranking step (placeholder) ---")
        # Need a reranking model here.
